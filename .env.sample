# ========================================
# ==============================================
# Supabase 設定
# ==============================================
SUPABASE_URL=your-supabase-url
SUPABASE_ANON_KEY=your-supabase-anon-key
SUPABASE_SERVICE_ROLE_KEY=your-supabase-service-role-key

# Supabase PostgreSQL接続文字列
# SupabaseダッシュボードのSettings > Database > Connection Stringから取得
# 形式: postgresql://postgres:[YOUR-PASSWORD]@db.[PROJECT-REF].supabase.co:5432/postgres
DATABASE_URL=postgresql://postgres:[YOUR-PASSWORD]@db.[PROJECT-REF].supabase.co:5432/postgres

# ========================================
# GLM API設定 (OpenAI-compatible)
# ========================================
# GLM API Key from https://open.bigmodel.cn/
GLM_API_KEY=your_glm_api_key_here
# GLM Model: GLM-4.5-Air (recommended), GLM-4.5, GLM-4-Flash, etc.
GLM_MODEL=GLM-4.5-Air
# GLM API Base URL (default: https://open.bigmodel.cn/api/paas/v4)
GLM_BASE_URL=https://open.bigmodel.cn/api/paas/v4
# Review Language: zh (Chinese), ja (Japanese), en (English)
REVIEW_LANGUAGE=zh

# ========================================
# バックエンド設定
# ========================================
BACKEND_PORT=3080
CORS_ORIGINS=http://localhost:3000
LOG_LEVEL=INFO

# ========================================
# フロントエンド設定
# ========================================
VITE_SUPABASE_URL=${SUPABASE_URL}
VITE_SUPABASE_ANON_KEY=${SUPABASE_ANON_KEY}
VITE_BACKEND_URL=http://localhost:3080

# ========================================
# faster-whisper設定 (GPU-accelerated transcription)
# ========================================
# Device: cuda (GPU) or cpu
FASTER_WHISPER_DEVICE=cuda
# Compute type: float16 (GPU, fastest), float32 (GPU, more accurate), int8 (CPU, quantized)
FASTER_WHISPER_COMPUTE_TYPE=float16
# Model size: large-v3-turbo (recommended), large-v3, medium, small, base, tiny
FASTER_WHISPER_MODEL_SIZE=large-v3-turbo
# Language: auto (detect), zh (Chinese), ja (Japanese), en (English), etc.
WHISPER_LANGUAGE=zh
# Number of worker threads for transcription (CPU: set to CPU cores, GPU: 4-8 recommended)
WHISPER_THREADS=4

# ========================================
# Audio Chunking (for faster transcription)
# ========================================
# Enable chunking for audio files longer than CHUNK_SIZE_MINUTES
ENABLE_CHUNKING=true
# Target chunk length in minutes (recommended: 5-10 for CPU, 10-15 for GPU)
CHUNK_SIZE_MINUTES=10
# Overlap duration in seconds to maintain context at boundaries
# Recommended: 15s with VAD splitting, 30s for fixed splitting
CHUNK_OVERLAP_SECONDS=15
# Maximum chunks to process in parallel
# For faster-whisper on GPU: 4-8 (based on VRAM)
MAX_CONCURRENT_CHUNKS=2
# Use Voice Activity Detection for smart splitting at silence points
USE_VAD_SPLIT=true
# Silence threshold in dB for VAD (lower = more sensitive)
VAD_SILENCE_THRESHOLD=-30
# Minimum silence duration in seconds to consider as split point
VAD_MIN_SILENCE_DURATION=0.5
# Merge strategy: "lcs" (text-based, recommended) or "timestamp" (simple)
MERGE_STRATEGY=lcs

# ========================================
# Audio Processing
# ========================================
# Number of audio files to process in parallel
AUDIO_PARALLELISM=1

# ========================================
# Data Retention (Auto-Delete)
# ========================================
# Maximum days to keep transcriptions before auto-delete
# Set to 0 to disable auto-delete
MAX_KEEP_DAYS=7
# Hour to run daily cleanup (24-hour format, e.g., 9 = 9 AM, 0 = midnight)
CLEANUP_HOUR=9

# ========================================
# Testing
# ========================================
# DISABLE_AUTH=true disables authentication (for development/testing only)
DISABLE_AUTH=false
