services:
  # PostgreSQL 18 Database (Alpine - Local Development)
  postgres:
    image: postgres:18-alpine
    container_name: whisper_postgres_dev
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_DB:-whisper_summarizer}
    volumes:
      - postgres_data_dev:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - whisper_network_dev

  # Nginx Reverse Proxy (Single Entry Point)
  nginx:
    image: nginx:alpine
    container_name: whisper_nginx_dev
    ports:
      - "${NGINX_PORT:-8130}:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
    environment:
      NGINX_HOST: ${NGINX_HOST:-localhost}
      NGINX_PORT: ${NGINX_PORT:-8130}
    depends_on:
      - server
      - frontend
    networks:
      - whisper_network_dev
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 10s
      timeout: 5s
      retries: 3

  # FastAPI Server (Lightweight - No GPU, No Whisper)
  # Port 8000 now internal only (accessed via nginx at /api)
  server:
    build:
      context: ./server
      dockerfile: Dockerfile
    container_name: whisper_server_dev
    environment:
      # Database (Local PostgreSQL)
      DATABASE_URL: postgresql+psycopg2://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/${POSTGRES_DB:-whisper_summarizer}
      POSTGRES_DB: ${POSTGRES_DB:-whisper_summarizer}
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}

      # Supabase Auth
      SUPABASE_URL: ${SUPABASE_URL}
      SUPABASE_ANON_KEY: ${SUPABASE_ANON_KEY}
      SUPABASE_SERVICE_ROLE_KEY: ${SUPABASE_SERVICE_ROLE_KEY}

      # GLM API (for AI chat feature)
      GLM_API_KEY: ${GLM_API_KEY}
      GLM_MODEL: ${GLM_MODEL:-GLM-4.7-Flash}
      GLM_BASE_URL: ${GLM_BASE_URL:-https://api.z.ai/api/paas/v4/}
      REVIEW_LANGUAGE: ${REVIEW_LANGUAGE:-zh}

      # Runner Authentication
      RUNNER_API_KEY: ${RUNNER_API_KEY:-dev-secret-key}

      # Server Config
      BACKEND_PORT: ${BACKEND_PORT:-3080}
      CORS_ORIGINS: ${CORS_ORIGINS:-http://localhost}
      PUBLIC_HOSTNAME: ${PUBLIC_HOSTNAME:-http://localhost:8130}
      LOG_LEVEL: ${LOG_LEVEL:-DEBUG}
      # DISABLE_AUTH removed - auth bypass is now hardcoded for localhost only
      TRUSTED_HOSTS: ${TRUSTED_HOSTS:-localhost,nginx}

      # Data Retention
      MAX_KEEP_DAYS: ${MAX_KEEP_DAYS:-7}
      CLEANUP_HOUR: ${CLEANUP_HOUR:-9}

      # Text Formatting (LLM-based) - 5000 byte chunks to avoid timeouts
      MAX_FORMAT_CHUNK: ${MAX_FORMAT_CHUNK:-5000}
    # No ports section - internal only
    volumes:
      # Source code mount for hot reload
      - ./server:/app
      # Data directory mount (server-specific data like transcriptions)
      - ./data/server:/app/data
      # Shared uploads directory (accessible by both server and runner)
      - ./data/uploads:/app/data/uploads
      # Logs directory
      - ./logs:/app/logs
    command: /bin/sh -c "uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload --log-level debug 2>&1 | tee /app/logs/server.log"
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - whisper_network_dev

  # GPU Runner (faster-whisper + GLM)
  runner:
    build:
      context: ./runner
      dockerfile: Dockerfile
    container_name: whisper_runner_dev
    environment:
      # Server Connection
      SERVER_URL: ${RUNNER_SERVER_URL:-http://server:8000}
      RUNNER_API_KEY: ${RUNNER_API_KEY:-dev-secret-key}
      RUNNER_ID: runner-dev

      # Polling Config
      POLL_INTERVAL_SECONDS: ${POLL_INTERVAL_SECONDS:-10}
      MAX_CONCURRENT_JOBS: ${MAX_CONCURRENT_JOBS:-2}

      # GLM API
      GLM_API_KEY: ${GLM_API_KEY}
      GLM_MODEL: ${GLM_MODEL:-GLM-4.7-Flash}
      GLM_BASE_URL: ${GLM_BASE_URL:-https://api.z.ai/api/paas/v4/}
      REVIEW_LANGUAGE: ${REVIEW_LANGUAGE:-zh}

      # faster-whisper Config (GPU-accelerated transcription)
      FASTER_WHISPER_DEVICE: ${FASTER_WHISPER_DEVICE:-cuda}
      FASTER_WHISPER_COMPUTE_TYPE: ${FASTER_WHISPER_COMPUTE_TYPE:-int8_float16}
      FASTER_WHISPER_MODEL_SIZE: ${FASTER_WHISPER_MODEL_SIZE:-large-v3-turbo}
      WHISPER_LANGUAGE: ${WHISPER_LANGUAGE:-auto}
      WHISPER_THREADS: ${WHISPER_THREADS:-4}

      # Audio Chunking (10-min chunks with timestamp-based merge)
      ENABLE_CHUNKING: ${ENABLE_CHUNKING:-true}
      CHUNK_SIZE_MINUTES: ${CHUNK_SIZE_MINUTES:-5}
      CHUNK_OVERLAP_SECONDS: ${CHUNK_OVERLAP_SECONDS:-15}
      MAX_CONCURRENT_CHUNKS: ${MAX_CONCURRENT_CHUNKS:-4}
      USE_VAD_SPLIT: ${USE_VAD_SPLIT:-false}
      VAD_SILENCE_THRESHOLD: ${VAD_SILENCE_THRESHOLD:--30}
      VAD_MIN_SILENCE_DURATION: ${VAD_MIN_SILENCE_DURATION:-0.5}
      MERGE_STRATEGY: ${MERGE_STRATEGY:-lcs}

      # Text Formatting (LLM-based) - 5000 byte chunks to avoid timeouts
      MAX_FORMAT_CHUNK: ${MAX_FORMAT_CHUNK:-5000}

      # Logging
      LOG_LEVEL: ${LOG_LEVEL:-DEBUG}
    volumes:
      # Source code mount for hot reload
      - ./runner:/app
      # Data directory mount (runner-specific data)
      - ./data/runner:/app/data
      # Shared uploads directory (accessible by both server and runner)
      - ./data/uploads:/app/data/uploads
      # Logs directory
      - ./logs:/app/logs
      # Whisper model cache
      - runner_cache:/tmp/whisper_models
    # GPU Support (faster-whisper with cuDNN)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    depends_on:
      - server
    networks:
      - whisper_network_dev

  # React Frontend (Development - Vite Dev Server)
  # Accessed via nginx at http://localhost:8130 (port 3000 internal only)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    container_name: whisper_frontend_dev
    environment:
      VITE_SUPABASE_URL: ${SUPABASE_URL}
      VITE_SUPABASE_ANON_KEY: ${SUPABASE_ANON_KEY}
      # Use relative path - nginx handles routing
      VITE_BACKEND_URL: ${VITE_BACKEND_URL:-/api}
      VITE_PUBLIC_URL: ${VITE_PUBLIC_URL:-http://localhost:8130}
    volumes:
      # Source code mount for hot reload
      - ./frontend:/app
      # Logs directory
      - ./logs:/app/logs
      # node_modules excluded (preserves container's node_modules)
      - /app/node_modules
      # Vite cache excluded
      - /app/.vite
    depends_on:
      - server
    networks:
      - whisper_network_dev

networks:
  whisper_network_dev:
    driver: bridge

volumes:
  postgres_data_dev:
    driver: local
  runner_cache:
    driver: local
