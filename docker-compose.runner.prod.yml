# ========================================
# Production Runner Deployment
# ========================================
# This file uses the pre-built Docker Hub image
# Use this for deploying on a separate GPU server
#
# Usage:
#   1. Copy .env.runner to .env.runner and fill in your values
#   2. Run: ./start_runner.sh

services:
  # GPU Runner (from Docker Hub)
  runner:
    # Use pre-built image from Docker Hub
    image: xyshyniaphy/whisper-summarizer-runner:latest

    container_name: whisper_runner_prod
    env_file:
      - .env.runner

    # GPU Support (faster-whisper with cuDNN)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

    volumes:
      # Runner data directory (transcription storage)
      - ${DATA_DIR:-./data/runner}:/app/data

      # Logs directory
      - ${LOG_DIR:-./logs}:/app/logs

      # Whisper model cache (persistent across restarts)
      - runner_cache:/tmp/whisper_models

    restart: unless-stopped

    healthcheck:
      test: ["CMD", "pgrep", "-f", "python.*poller"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 30s

volumes:
  runner_cache:
    driver: local
